{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "subs_dir = 'subtitles'\n",
    "vids_dir = 'videos'\n",
    "subs_files = sorted(os.listdir(subs_dir))\n",
    "vids_files = [x for x in sorted(os.listdir(vids_dir)) if x[-4:] == '.m4v']\n",
    "print(subs_files)\n",
    "print(vids_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "\n",
    "subs = []\n",
    "for subs_file in subs_files:\n",
    "    subs.append(webvtt.read(os.path.join(subs_dir, subs_file)))\n",
    "\n",
    "for caption in subs[0]:\n",
    "    print('start:', caption.start)\n",
    "    print('caption:', caption.text)\n",
    "    print('end:', caption.end)\n",
    "    print('----------------')\n",
    "\n",
    "print(subs[6][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The data uses middle minute of the videos, thus we need to remove captions that don't belong\n",
    "to that timeframe. Also there should be a caption per each .5 sec in the video to match eeg data rows.\n",
    "\"\"\"\n",
    "from moviepy.editor import VideoFileClip  # This library is complete overkill for this purpose\n",
    "import time\n",
    "     \n",
    "# Read video file durations\n",
    "video_durations = [] \n",
    "for videofile in vids_files:\n",
    "    clip = VideoFileClip(os.path.join(vids_dir, videofile))\n",
    "    print(videofile, clip.duration)\n",
    "    video_durations.append(clip.duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eeg_df = pd.read_csv('EEG_data.csv')\n",
    "print(eeg_df.iloc[0])\n",
    "eeg_df.VideoID = pd.to_numeric(eeg_df.VideoID)\n",
    "eeg_df.SubjectID = pd.to_numeric(eeg_df.SubjectID)\n",
    "print(eeg_df.VideoID.value_counts().sort_index())\n",
    "print(eeg_df.SubjectID.value_counts().sort_index())\n",
    "\n",
    "# Users seem to have different amounts of watch time assuming .5 second measurement interval is correct \n",
    "for subject_id in sorted(eeg_df.SubjectID.unique()):\n",
    "    print('SubjecttId', subject_id)\n",
    "    print(eeg_df.query('SubjectID == {}'.format(subject_id)).VideoID.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter captions from subs that are not in the middle minute\n",
    "import numpy as np\n",
    "\n",
    "def invalid_sub(sub, duration):\n",
    "    start_time = time.strftime('%H:%M:%S', time.gmtime(duration / 2 - 30))\n",
    "    end_time = time.strftime('%H:%M:%S', time.gmtime(duration / 2 + 30))\n",
    "    return sub.end < start_time or sub.start > end_time\n",
    "\n",
    "#filtered_subs = [list(filter(lambda x: not invalid_sub(x, duration), sub)) \n",
    "#                 for sub, duration in zip(subs, video_durations)]\n",
    "\n",
    "def strf_seconds(seconds):\n",
    "    return '{:2.0f}:{:2.0f}:{:2.0f}.{}'.format(seconds / 3600, int(int(seconds) / 60) % 60, int(seconds) % 60,\n",
    "                                         str(seconds).split('.')[-1]).replace(' ', '0')\n",
    "\n",
    "def get_video_intervals(vid_duration, n_rows):\n",
    "    half_usertime = n_rows / 4  # 1 row is .5 seconds\n",
    "    start_time = vid_duration / 2 - half_usertime\n",
    "    end_time = vid_duration / 2 + half_usertime\n",
    "\n",
    "    intervals = np.arange(2 * start_time, 2 * end_time) / 2\n",
    "    if len(intervals) > n_rows: intervals = intervals[:-1]\n",
    "    assert len(intervals) == n_rows\n",
    "    return [strf_seconds(interval) for interval in intervals]\n",
    "\n",
    "def get_subs_for_rows(vid_subs, vid_intervals):\n",
    "    #print(vid_intervals)\n",
    "    row_subs = []\n",
    "    a = 0\n",
    "    b = 0\n",
    "    #print(len(vid_intervals), len(vid_subs))\n",
    "    while b < len(vid_intervals):\n",
    "        while a < len(vid_subs) and vid_subs[a].end < vid_intervals[b]:\n",
    "            a += 1\n",
    "        if a < len(vid_subs) and vid_subs[a].start < vid_intervals[b] < vid_subs[a].end: \n",
    "            row_subs.append(vid_subs[a].text)\n",
    "        else: row_subs.append('<empty>')\n",
    "        b += 1  \n",
    "    assert len(vid_intervals) == len(row_subs)\n",
    "    return row_subs\n",
    "\n",
    "# Test strf_seconds\n",
    "print(strf_seconds(119.5))\n",
    "#for i, row in eeg_df.iterrows():\n",
    "#    print(row.VideoID, row.SubjectID, sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make on row in subs as caption for a .5 sec period (as if a sample is taken every .5 seconds)\n",
    "subs_per_row = []\n",
    "for subject_id in sorted(eeg_df.SubjectID.unique()):\n",
    "    for video_id in sorted(eeg_df.VideoID.unique()):\n",
    "        #print(subject_id, video_id)\n",
    "        video_duration = video_durations[int(video_id)]\n",
    "        video_intervals = get_video_intervals(video_duration,\n",
    "                                              len(eeg_df.query('SubjectID == {} and VideoID == {}'\n",
    "                                                              .format(subject_id, video_id))))\n",
    "        subs_per_row += get_subs_for_rows(subs[int(video_id)], video_intervals)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo embed subs, requires https://github.com/HIT-SCIR/ELMoForManyLangs\n",
    "from elmoformanylangs import Embedder\n",
    "import numpy as np\n",
    "e = Embedder('../../../text_embedding_repos/ELMoForManyLangs/Elmo_english_pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subs_sents = [sub.split() for sub in subs_per_row]\n",
    "elmo_sents = e.sents2elmo(subs_sents)\n",
    "elmo_avg_sents = [np.mean(vec, axis=0) for vec in elmo_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for i, row in eeg_df.iterrows():\n",
    "    row = np.insert(elmo_avg_sents[i], (0, 0), (row.SubjectID, row.VideoID)).astype(np.float32)\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data, columns=['SubjectID', 'VideoID', *[str(x) for x in range(len(elmo_avg_sents[0]))]])\n",
    "#df.to_csv('elmo_embedded_subs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('elmo_embedded_subs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole df takes 257M space -> split by VideoID\n",
    "for video_id in sorted(df.VideoID.unique()):\n",
    "    vid_df = df.query('VideoID == {}'.format(video_id))\n",
    "    vid_df.to_csv('vid_{}_elmo_embedded_subs.csv'.format(int(video_id)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining vid_dfs\n",
    "import numpy as np\n",
    "vid_dfs = pd.concat([pd.read_csv('vid_{}_elmo_embedded_subs.csv'.format(i))\n",
    "                     for i in range(10)], ignore_index=True\n",
    "                   ).sort_values(['SubjectID', 'VideoID']).reset_index(drop=True)\n",
    "\n",
    "vec_cols = [str(x) for x in range(1024)]\n",
    "vid_dfs[vec_cols] = vid_dfs[vec_cols].astype(np.float32) # for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round5(x):\n",
    "    return round(x, 5)\n",
    "\n",
    "assert vid_dfs.applymap(round5).equals(df.applymap(round5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
