{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ktia-99qlzOJ"
   },
   "source": [
    "Running requires having the data on Google Drive or uploading the data to Colab via the left menu and setting the data path correctly.\n",
    "\n",
    "To use TPU, set \"tpu\" to True and hardware accelerator to \"TPU\" from Edit -> Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePT0I-SulzyD"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tpu = False\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "data_path = '/gdrive/My Drive/Colab Notebooks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1MXnzbScS6B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
    "if tpu:\n",
    "  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def tpu_compatibilitate(model):\n",
    "  if tpu: return tf.contrib.tpu.keras_to_tpu_model(\n",
    "      model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "              tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
    "  else: return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIdDWtwHlstY"
   },
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "s7bgJDMUOQKN",
    "outputId": "269b4347-ba9e-48ed-d487-04628e739b5c"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + 'EEG_data.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3igFo2wZz4H"
   },
   "source": [
    "The labels for confusion are the same for each video by subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xR2ziDnpY2KP"
   },
   "outputs": [],
   "source": [
    "for subjId in set(data.SubjectID):\n",
    "  for vidId in set(data.VideoID):\n",
    "    assert data.query('SubjectID == {} and VideoID == {}'\n",
    "                     .format(subjId, vidId))['user-definedlabeln'].mean() in (0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XS39to7ylnxY"
   },
   "source": [
    "Load subtitle vectors  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZ4BFXIZVfYa"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "from csv\n",
    "\"\"\"\n",
    "#vid_dfs = pd.concat([pd.read_csv(notebook_path + 'subtitles/vid_{}_elmo_embedded_subs.csv'.format(i))\n",
    "#                     for i in range(10)], ignore_index=True\n",
    "#                   ).sort_values(['SubjectID', 'VideoID']).reset_index(drop=True)\n",
    "\n",
    "#vec_cols = [str(x) for x in range(1024)]\n",
    "\n",
    "#sub_vecs = vid_dfs[vec_cols].values.astype('float32')\n",
    "\n",
    "\"\"\"\n",
    "save/load from npy\n",
    "\"\"\"\n",
    "sub_vec_path = data_path + 'subtitle_vecs.npy'\n",
    "#np.save(sub_vec_path, sub_vecs)\n",
    "sub_vecs = np.load(sub_vec_path)\n",
    "sub_vec_dim = sub_vecs.shape[1]\n",
    "\n",
    "\"\"\"\n",
    "Make a dataset of original data combined with sub vecs \n",
    "\"\"\"\n",
    "dataset = np.hstack((data.values.astype('float32'), sub_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPnuOcx-lcHN"
   },
   "source": [
    "PCA to reduce subtitle vector dimensions. Speeds up training and also has the potential to increase performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4CpmAOtgaKI"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\"\"\"\n",
    "PCA to reduce dimension of the word average vectors (might give better results)\n",
    "\"\"\"\n",
    "sub_vec_dim = 12\n",
    "pca = PCA(n_components=sub_vec_dim)\n",
    "pcad_sub_vecs = pca.fit_transform(sub_vecs)\n",
    "#dataset = np.hstack((data.values.astype('float32'), pcad_sub_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rKcDQdVTseQI",
    "outputId": "bf8f59b8-65db-4dcc-e068-030a3c7f76ac"
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7B9Ra-blV4T"
   },
   "source": [
    "Preprocessing as is done in https://github.com/mehmani/DNNs-for-EEG-Signals/blob/master/DNNforEEFSignals.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KUxzySen19sD",
    "outputId": "0d360b98-d058-4137-ee62-e0ec2cf9638a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def NormSignal(S, I):\n",
    "    #normalize features\n",
    "    S=S.reshape(-1, 1)\n",
    "    if I not in [0, 1, 13, 14]:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(S)\n",
    "        scaled = scaled\n",
    "    else:\n",
    "        scaled = S\n",
    "    return scaled.reshape(-1).tolist()\n",
    "\n",
    "NormDataG = np.array([NormSignal(dataset[:,i], i) for i in range(dataset.shape[1])]).T\n",
    "print(NormDataG.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MR5eZ3s7lK90"
   },
   "source": [
    "Additional metrics besides accuracy to have more information on model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "BWzd23ZwzndW",
    "outputId": "f35db6ad-016e-4d07-f266-13dc51f89f79"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def swap_tensor_binary_ints(x):\n",
    "    return tf.math.add(tf.negative(x), 1.)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(\"Test swapping tensor binary ints\")\n",
    "  diagonal = tf.linalg.diag(tf.ones((1, 2)))\n",
    "  print('orig:\\n', diagonal.eval())\n",
    "  print('swap:\\n', swap_tensor_binary_ints(diagonal).eval())\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true =y_true\n",
    "    y_pred = tf.round(y_pred) # implicit 0.5 threshold via tf.round\n",
    "    y_correct = y_true * y_pred\n",
    "    sum_true = tf.reduce_sum(y_true, axis=1)\n",
    "    sum_pred = tf.reduce_sum(y_pred, axis=1)\n",
    "    sum_correct = tf.reduce_sum(y_correct, axis=1)\n",
    "    precision = sum_correct / sum_pred\n",
    "    recall = sum_correct / sum_true\n",
    "    f_score = 2 * precision * recall / (precision + recall)\n",
    "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
    "    return tf.reduce_mean(f_score)\n",
    "  \n",
    "def f1_flipped(y_true, y_pred):\n",
    "    y_true = y_true\n",
    "    y_pred = tf.round(y_pred) # implicit 0.5 threshold via tf.round\n",
    "    return f1_score(swap_tensor_binary_ints(y_true), \n",
    "                    swap_tensor_binary_ints(y_pred))\n",
    "\n",
    "def rocauc(y_true, y_pred):\n",
    "    roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "fXrP2NC1LI57",
    "outputId": "cb65cd25-dc60-43a3-ab53-577f2a00b1ee"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.python.keras.layers import Input, LSTM, Bidirectional, Dense, Flatten, Dropout, TimeDistributed, Conv2D, MaxPooling2D, Masking\n",
    "\n",
    "\"\"\"\n",
    "Model from the paper \"confused or not confused\"\n",
    "\"\"\"\n",
    "def get_model_timedist(intervals, n_dim=11):\n",
    "    model = Sequential([\n",
    "        #Masking(mask_value=0, input_shape=(A, n_dim)), # Masking does not help for some reason (should help with padded data?)\n",
    "        BatchNormalization(input_shape=(intervals, n_dim), axis=2), # New version of keras doesn't support \"mode\" attribute, which was used in the original code (mode=0)\n",
    "        Bidirectional(LSTM(50, return_sequences=False, activation='selu'), input_shape=(intervals, n_dim)),\n",
    "        Dense(intervals, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='RMSprop',\n",
    "                  metrics=['binary_accuracy', f1_score, f1_flipped])\n",
    "    return model, (-1, intervals, n_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qF7LfkrvpA1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model from the paper \"confused or not confused\" with binary output per data point\n",
    "\n",
    "\"\"\"\n",
    "def get_model(intervals, n_dim=11):\n",
    "    model = Sequential([\n",
    "        #Masking(mask_value=0, input_shape=(A, n_dim)), # Masking does not help for some reason (should help with padded data?)\n",
    "        BatchNormalization(input_shape=(intervals, n_dim), axis=2),\n",
    "        Bidirectional(LSTM(50, return_sequences=False, activation='selu'), input_shape=(intervals, n_dim)),\n",
    "        #Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adagrad',\n",
    "                  metrics=['acc', f1_score, f1_flipped])\n",
    "    return model, (-1, intervals, n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C56mNBEsON09"
   },
   "outputs": [],
   "source": [
    "def get_mehmani_model(intervals, n_dim=11):\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv2D(20, (5,5), activation='relu'),\n",
    "              input_shape=(1, intervals, n_dims, 1)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(10, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "    model.add(LSTM(10))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_score, f1_flipped])\n",
    "    return model, (-1, 1, intervals, n_dim, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGzLNvsWiJEQ"
   },
   "source": [
    "Functions for making the data amount of intervals the same for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9C1tx41gY163"
   },
   "outputs": [],
   "source": [
    "def min_max_rows_per_subject_vid(X):\n",
    "  VideoID = list(set(X[:,1]))\n",
    "  SubjectID = list(set(X[:,0]))\n",
    "\n",
    "  max_intervals = 0 # length of signal\n",
    "  min_intervals = len(X)\n",
    "  \n",
    "  for subId in SubjectID:\n",
    "      for vidId in VideoID:\n",
    "          X_tmp=X[(X[:, 0] == subId) & (X[:, 1] == vidId)]\n",
    "          max_intervals = max(len(X_tmp), max_intervals)\n",
    "          min_intervals = min(len(X_tmp), min_intervals)\n",
    "  print(max_intervals)\n",
    "  print(min_intervals)\n",
    "  assert max_intervals == 144\n",
    "  return min_intervals, max_intervals\n",
    "\n",
    "min_intervals, max_intervals = min_max_rows_per_subject_vid(dataset)\n",
    "\n",
    "\n",
    "def zero_pad_data(X, max_intervals, y_col):\n",
    "  # Manual Padding to fixed size:\n",
    "    X_pad = None\n",
    "    VideoID = list(set(X[:,1]))\n",
    "    SubjectID = list(set(X[:,0])) \n",
    "    for subId in SubjectID:\n",
    "        for vidId in VideoID:\n",
    "            X_sv = X[(X[:,0]==subId) & (X[:,1]==vidId)]\n",
    "            pad_len = max_intervals - X_sv.shape[0]\n",
    "            \n",
    "            z = np.zeros((pad_len, X_sv.shape[1]), dtype=X_sv.dtype)\n",
    "            z[:,0] = X_sv[:,0][pad_len]\n",
    "            z[:,1] = X_sv[:,1][pad_len]\n",
    "            z[:,y_col] = X_sv[:,y_col][pad_len]\n",
    "            \n",
    "            X_sv_pad = np.concatenate((X_sv, z), axis=0)\n",
    "            X_sv_pad = X_sv_pad.reshape(1, max_intervals, -1)\n",
    "\n",
    "            X_pad = X_sv_pad if X_pad is None else np.vstack((X_pad,X_sv_pad))\n",
    "            \n",
    "    return X_pad\n",
    "\n",
    "def truncate_data(X, min_intervals, y_col):\n",
    "    X_trunc = None\n",
    "    VideoID = list(set(X[:,1]))\n",
    "    SubjectID = list(set(X[:,0]))\n",
    "    for vidId in VideoID:\n",
    "      for subId in SubjectID:\n",
    "          X_sv = X[(X[:,0]==subId) & (X[:,1]==vidId)]\n",
    "          trunc_len = min_intervals\n",
    "          X_sv_trunc = X_sv[0:trunc_len].reshape(1, min_intervals, -1)\n",
    "          X_trunc = X_sv_trunc if X_trunc is None else np.vstack((X_trunc, X_sv_trunc))\n",
    "    return X_trunc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0L2NggfwhVsj"
   },
   "source": [
    "Define target variable and which variables to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1roletOchUkD"
   },
   "outputs": [],
   "source": [
    "\n",
    "y_col = 14 # The student's confusion column\n",
    "orig_train_data_cols = list(range(2,14))\n",
    "vector_cols = list(np.arange(sub_vec_dim) + 15) \n",
    "\n",
    "train_cols = orig_train_data_cols + vector_cols\n",
    "n_dim = len(train_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDoCgF4Yhx7T"
   },
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dGDbJpDS_Pv"
   },
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "\n",
    "def train_eval_model(model, X_train, y_train, X_test, y_test, train_cols, intervals,\n",
    "                     epochs=20, batch_size=20, verbose=1): \n",
    "    start = time()\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), verbose=verbose, shuffle=True)\n",
    "    print('model trained in {:.3f} seconds'.format(time() - start))\n",
    "\n",
    "    loss, acc, f1, f1_flip = model.evaluate(X_test, y_test, verbose=0)\n",
    "    #y_pred = model.predict(X_test).ravel()\n",
    "    #roc_auc = rocauc(y_test, y_pred)\n",
    "    return (acc, f1, f1_flip, history)\n",
    "\n",
    " \n",
    "def cross_validate(model, X_shape, data, intervals, even_data, n_test=2,\n",
    "                   time_distributed=False, verbose=1, epochs=50, batch_size=20):\n",
    "  \"\"\"\n",
    "  even_data: either truncate_data or zero_pad_data to make number of intervals\n",
    "             even for each data point\n",
    "  \"\"\"\n",
    "  results = []\n",
    "  initial_weights = model.get_weights()\n",
    "\n",
    "  for i in range(0, 10, n_test):\n",
    "    model.set_weights(initial_weights) # Reset weights to forget training done on current iteration's test data  \n",
    "    \n",
    "    data_train = even_data(data[np.in1d(data[:,0], (i, i+1), invert=True)], intervals, y_col=y_col)\n",
    "    data_test = even_data(data[np.in1d(data[:,0], (i, i+1))], intervals, y_col=y_col)\n",
    "    X_train = data_train[:, :, train_cols]\n",
    "    y_train = data_train[:, :, y_col]\n",
    "    X_test = data_test[:, :, train_cols]\n",
    "    y_test = data_test[:, :, y_col]\n",
    "\n",
    "    X_train = X_train.reshape(X_shape)\n",
    "    print('Xtrain shape', X_train.shape)\n",
    "    X_test = X_test.reshape(X_shape)\n",
    "    print('Xtest shape', X_test.shape)\n",
    "    \n",
    "    if not time_distributed: y_train = y_train.reshape(-1, intervals).mean(axis=1)\n",
    "    print('ytrain shape', y_train.shape)\n",
    "    if not time_distributed: y_test = y_test.reshape(-1, intervals).mean(axis=1)\n",
    "    print('ytest shape', y_test.shape)\n",
    "\n",
    "    start = time()\n",
    "    print('{}-fold cross validation, iteration {}'\n",
    "          .format(int(10/n_test), len(results) +1))\n",
    "    acc, f1, f1_flip, history = train_eval_model(model, X_train, y_train,\n",
    "                                                 X_test, y_test,\n",
    "                                                 train_cols, intervals,\n",
    "                                                 epochs=epochs, batch_size=batch_size,\n",
    "                                                 verbose=verbose)\n",
    "    \n",
    "    results.append({'acc': acc, 'F1': f1, 'F1-flipped': f1_flip})\n",
    "    \n",
    "    print('current cross-validation mean accuracy: {:.3f}, f1: {:.3f}, and f1 flipped: {:.3f}'.format(\n",
    "          *[np.mean([r[key] for r in results]) for key in results[0].keys()]))\n",
    "    print('cross-validation total time: {:.3f} seconds'.format(time() - start))\n",
    "\n",
    "  return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0sjlQWCQUjPb",
    "outputId": "c9605b7f-1606-4fbc-8b30-7013b62c3816"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Mehmani's model \n",
    "\"\"\"\n",
    "#model, input_shape = get_mehmani_model(max_intervals, n_dims)\n",
    "#results = cross_validate(model=model, X_shape=input_shape, data=NormDataG,\n",
    "#                         intervals=max_intervals, even_data=zero_pad_data, n_test=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vD1kWnUNJonX"
   },
   "outputs": [],
   "source": [
    "#print(*results, sep='\\n')\n",
    "#print('cross-validation mean accuracy: {:.3f}, f1: {:.3f}, and f1 flipped: {:.3f}'.format(\n",
    "#       *[np.mean([r[key] for r in results]) for key in results[0].keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5308
    },
    "colab_type": "code",
    "id": "C1wWBL4WTNG3",
    "outputId": "731d214b-1dba-4ae3-da3d-077acb2f8a93"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the model from the paper \"Confused or not confused\"\n",
    "\"\"\"\n",
    "train_cols = orig_train_data_cols\n",
    "n_dim = len(train_cols)\n",
    "model, input_shape = get_model_timedist(min_intervals, n_dim)\n",
    "tpu_model = tpu_compatibilitate(model)\n",
    "\n",
    "results = cross_validate(model=tpu_model, X_shape=input_shape, data=dataset,\n",
    "                         time_distributed=True,\n",
    "                         intervals=min_intervals, even_data=truncate_data,\n",
    "                         n_test=2, verbose=1, epochs=25, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "L7awQkiuTQl0",
    "outputId": "27e73c34-8c7c-47f8-a7ca-1b2bad9d2efa"
   },
   "outputs": [],
   "source": [
    "print(*results, sep='\\n')\n",
    "print('cross-validation mean accuracy: {:.3f}, f1: {:.3f}, and f1 flipped: {:.3f}'.format(\n",
    "       *[np.mean([r[key] for r in results]) for key in results[0].keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5308
    },
    "colab_type": "code",
    "id": "cLxDiqS2ohuY",
    "outputId": "23f58101-941a-48a6-e2f2-6740016fa138"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the model from the paper \"Confused or not confused\" with sub vectiors\n",
    "\"\"\"\n",
    "train_cols = orig_train_data_cols + vector_cols\n",
    "n_dim = len(train_cols)\n",
    "model, input_shape = get_model_timedist(min_intervals, n_dim)\n",
    "tpu_model = tpu_compatibilitate(model)\n",
    "\n",
    "results = cross_validate(model=tpu_model, X_shape=input_shape, data=dataset,\n",
    "                         time_distributed=True,\n",
    "                         intervals=min_intervals, even_data=truncate_data,\n",
    "                         n_test=2, verbose=1, epochs=25, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9792
    },
    "colab_type": "code",
    "id": "0DUIzt97eVaM",
    "outputId": "2f777188-3994-496c-c02a-9406bd9f0302"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test mehmani model with subtitle vectors\n",
    "\"\"\"\n",
    "print(NormDataG.shape)\n",
    "train_cols = vector_cols\n",
    "n_dims = len(train_cols)\n",
    "model, input_shape = get_mehmani_model(max_intervals, n_dims)\n",
    "\n",
    "tpu_model = tpu_compatibilitate(model)\n",
    "results = cross_validate(model=tpu_model, X_shape=input_shape, data=NormDataG,\n",
    "                         intervals=max_intervals, even_data=zero_pad_data, n_test=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "Vwyd6soYf2BM",
    "outputId": "8ec66686-ef6e-49d2-83e5-4f161ce8a1f7"
   },
   "outputs": [],
   "source": [
    "print(*results, sep='\\n')\n",
    "print('cross-validation mean accuracy: {:.3f}, f1: {:.3f}, and f1 flipped: {:.3f}'.format(\n",
    "       *[np.mean([r[key] for r in results]) for key in results[0].keys()]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EDM_EEG_subtitlevecs.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
